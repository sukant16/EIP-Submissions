{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E3P2Assignment2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukant16/EIP-Submissions/blob/master/E3P2Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF_NGZ5P3f2y",
        "colab_type": "code",
        "outputId": "ab6bcd80-b112-4a57-b4b4-77e551a5cc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0BrWk0G-3-s",
        "colab_type": "code",
        "outputId": "7aa98df9-a527-45fa-aa00-e1c5e13b4fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Masking, LSTM, InputLayer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils, Sequence\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weia5vQz_MTR",
        "colab_type": "code",
        "outputId": "d6a525e1-5134-4f98-bb7c-8492d322740d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW5qEWcB3UcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/EIP3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1huxkZzo-7hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "filename = \"alice_in_wonderland.txt\"\n",
        "with open(filename, 'r') as f:\n",
        "  text = f.read().lower().replace(\"\\n\", \" \")\n",
        "\n",
        "# removing punctuations   \n",
        "text = text.split(\".\")\n",
        "text = [re.sub(r'[^\\w\\s]+', \"\", line) for line in text]\n",
        "raw_text = [re.sub(r' +', \" \", line) for line in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2LSDMP2-7dN",
        "colab_type": "code",
        "outputId": "68b04415-3cc7-4a13-aca3-3f7a4991870d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "raw_text[:4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project gutenbergs alices adventures in wonderland by lewis carroll this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever',\n",
              " ' you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at www',\n",
              " 'gutenberg',\n",
              " 'org title alices adventures in wonderland author lewis carroll posting date june 25 2008 ebook 11 release date march 1994 last updated october 6 2016 language english character set encoding utf8 start of this project gutenberg ebook alices adventures in wonderland alices adventures in wonderland lewis carroll the millennium fulcrum edition 3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xX9XA15-7Yr",
        "colab_type": "code",
        "outputId": "a7fbdbc6-1d32-4d60-9462-6abad85ef0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "chars = sorted(list({char for line in raw_text for char in line}))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = sum(map(len, raw_text))\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  152579\n",
            "Total Vocab:  38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1mzIHnJC01Y",
        "colab_type": "code",
        "outputId": "a0fc71d1-cc48-4daa-f51c-e8a7863fb2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# removing those sentences where there are only 1 or 2 characters\n",
        "raw_text = [line for line in raw_text if len(line)>2]\n",
        "min(map(len, raw_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN_A1BqV-7O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "First, I tried that each batch should have some fixed number of sentences and \n",
        "then converting them into sequences. However, despite all the hacks I could try, \n",
        "it always went OOM due to huge matrix size for some batches. \n",
        "So, below, I have taken fixed length of 100, taking character one by one till \n",
        "the point I reach the 100th character in the sentence (I am pre-padding those \n",
        "sequences where the length is less than 100) and after 100th character I start \n",
        "taking sliding window of 100.  \n",
        "'''\n",
        "seq_len = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for line in raw_text:\n",
        "  if len(line) <= 100:\n",
        "#     char_list = list(line)\n",
        "    for i in range(1, len(line)):\n",
        "      seq_in = line[:i]\n",
        "      seq_out = line[i]\n",
        "      dataX.append([char_to_int[char] for char in seq_in])\n",
        "      dataY.append(char_to_int[seq_out])\n",
        "  else:\n",
        "#     num_lines = np.ceil(len(line)/seq_len)\n",
        "    for i in range(1, len(line)):\n",
        "      if i <= 100:\n",
        "        seq_in = line[:i]\n",
        "        seq_out = line[i]\n",
        "      else:\n",
        "        seq_in = line[(i-seq_len):i]\n",
        "        seq_out = line[i]\n",
        "      dataX.append([char_to_int[char] for char in seq_in])\n",
        "      dataY.append(char_to_int[seq_out])\n",
        "dataX = pad_sequences(dataX, maxlen=seq_len, padding='pre', value=0)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KawdMfxnLu0V",
        "colab_type": "code",
        "outputId": "bc6de0f6-ae79-402d-b1e1-f1732630c90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(dataX), len(dataX[1]))\n",
        "print(len(dataY))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "151352 100\n",
            "151352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AF0MfPELSIq",
        "colab_type": "code",
        "outputId": "44273835-7dff-49d5-fb27-6a73e262007e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X = np.reshape(dataX, (len(dataX), seq_len, 1))\n",
        "X = X/float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(151352, 100, 1)\n",
            "(151352, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29EKMTtzDaf6",
        "colab_type": "code",
        "outputId": "f1912688-871d-488d-cd42-72b6b44ce999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 04:54:35.550185 140566800570240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 04:54:35.589604 140566800570240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 04:54:35.596701 140566800570240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 04:54:35.606682 140566800570240 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 04:54:35.624747 140566800570240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_1 (Dropout)          (None, 100, 1)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 38)                9766      \n",
            "=================================================================\n",
            "Total params: 799,270\n",
            "Trainable params: 799,270\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VboA_GkwDdO2",
        "colab_type": "code",
        "outputId": "c21c3d29-7389-43b8-ff02-56fc200b46c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "100 epochs are covered in three parts to ensure I don't lose trained model \n",
        "'''\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'assgn2_saved_models')\n",
        "model_name = 'lstm_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='loss',\n",
        "                             verbose=1, mode='auto',\n",
        "                             save_best_only=True, period=5)\n",
        "\n",
        "callback_list = [checkpoint]\n",
        "model.fit(X, y, batch_size=128, epochs=25, callbacks=callback_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "151352/151352 [==============================] - 322s 2ms/step - loss: 2.6852\n",
            "Epoch 2/25\n",
            "151352/151352 [==============================] - 316s 2ms/step - loss: 2.4699\n",
            "Epoch 3/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 2.3088\n",
            "Epoch 4/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 2.2084\n",
            "Epoch 5/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 2.1343\n",
            "\n",
            "Epoch 00005: loss improved from inf to 2.13429, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.005.h5\n",
            "Epoch 6/25\n",
            "151352/151352 [==============================] - 316s 2ms/step - loss: 2.0915\n",
            "Epoch 7/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 2.0251\n",
            "Epoch 8/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 1.9885\n",
            "Epoch 9/25\n",
            "151352/151352 [==============================] - 313s 2ms/step - loss: 1.9546\n",
            "Epoch 10/25\n",
            "151352/151352 [==============================] - 314s 2ms/step - loss: 1.9179\n",
            "\n",
            "Epoch 00010: loss improved from 2.13429 to 1.91787, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.010.h5\n",
            "Epoch 11/25\n",
            "151352/151352 [==============================] - 314s 2ms/step - loss: 1.8882\n",
            "Epoch 12/25\n",
            "151352/151352 [==============================] - 313s 2ms/step - loss: 1.8621\n",
            "Epoch 13/25\n",
            "151352/151352 [==============================] - 311s 2ms/step - loss: 1.8395\n",
            "Epoch 14/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.8175\n",
            "Epoch 15/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.7943\n",
            "\n",
            "Epoch 00015: loss improved from 1.91787 to 1.79427, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.015.h5\n",
            "Epoch 16/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.7792\n",
            "Epoch 17/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.7601\n",
            "Epoch 18/25\n",
            "151352/151352 [==============================] - 313s 2ms/step - loss: 1.7409\n",
            "Epoch 19/25\n",
            "151352/151352 [==============================] - 313s 2ms/step - loss: 1.7229\n",
            "Epoch 20/25\n",
            "151352/151352 [==============================] - 310s 2ms/step - loss: 1.7105\n",
            "\n",
            "Epoch 00020: loss improved from 1.79427 to 1.71049, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.020.h5\n",
            "Epoch 21/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 1.6969\n",
            "Epoch 22/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.6815\n",
            "Epoch 23/25\n",
            "151352/151352 [==============================] - 308s 2ms/step - loss: 1.6761\n",
            "Epoch 24/25\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.6604\n",
            "Epoch 25/25\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.6525\n",
            "\n",
            "Epoch 00025: loss improved from 1.71049 to 1.65251, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.025.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7e1fa0198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFlLxx3rTOm7",
        "colab_type": "code",
        "outputId": "fc73eee4-bde2-4402-81d0-d6c1f284b01e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# training further for 50 epochs\n",
        "model.fit(X, y, batch_size=128, epochs=50, callbacks=callback_list)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.6384\n",
            "Epoch 2/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.6282\n",
            "Epoch 3/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.6218\n",
            "Epoch 4/50\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.6126\n",
            "Epoch 5/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.6037\n",
            "\n",
            "Epoch 00005: loss improved from 1.65251 to 1.60372, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.005.h5\n",
            "Epoch 6/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5928\n",
            "Epoch 7/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5814\n",
            "Epoch 8/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.5716\n",
            "Epoch 9/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.5668\n",
            "Epoch 10/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.5645\n",
            "\n",
            "Epoch 00010: loss improved from 1.60372 to 1.56453, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.010.h5\n",
            "Epoch 11/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5546\n",
            "Epoch 12/50\n",
            "151352/151352 [==============================] - 300s 2ms/step - loss: 1.5477\n",
            "Epoch 13/50\n",
            "151352/151352 [==============================] - 302s 2ms/step - loss: 1.5450\n",
            "Epoch 14/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.5359\n",
            "Epoch 15/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5294\n",
            "\n",
            "Epoch 00015: loss improved from 1.56453 to 1.52936, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.015.h5\n",
            "Epoch 16/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.5270\n",
            "Epoch 17/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5133\n",
            "Epoch 18/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5094\n",
            "Epoch 19/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5086\n",
            "Epoch 20/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.4994\n",
            "\n",
            "Epoch 00020: loss improved from 1.52936 to 1.49942, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.020.h5\n",
            "Epoch 21/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.4934\n",
            "Epoch 22/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.4902\n",
            "Epoch 23/50\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.4839\n",
            "Epoch 24/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.4787\n",
            "Epoch 25/50\n",
            "151352/151352 [==============================] - 309s 2ms/step - loss: 1.4771\n",
            "\n",
            "Epoch 00025: loss improved from 1.49942 to 1.47711, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.025.h5\n",
            "Epoch 26/50\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.4714\n",
            "Epoch 27/50\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.4629\n",
            "Epoch 28/50\n",
            "151352/151352 [==============================] - 307s 2ms/step - loss: 1.4622\n",
            "Epoch 29/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.4546\n",
            "Epoch 30/50\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.4562\n",
            "\n",
            "Epoch 00030: loss improved from 1.47711 to 1.45619, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.030.h5\n",
            "Epoch 31/50\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.4476\n",
            "Epoch 32/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.4481\n",
            "Epoch 33/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.5696\n",
            "Epoch 34/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.4471\n",
            "Epoch 35/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.4300\n",
            "\n",
            "Epoch 00035: loss improved from 1.45619 to 1.42998, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.035.h5\n",
            "Epoch 36/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.4306\n",
            "Epoch 37/50\n",
            "151352/151352 [==============================] - 301s 2ms/step - loss: 1.4299\n",
            "Epoch 38/50\n",
            "151352/151352 [==============================] - 300s 2ms/step - loss: 1.4245\n",
            "Epoch 39/50\n",
            "151352/151352 [==============================] - 300s 2ms/step - loss: 1.4195\n",
            "Epoch 40/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.4192\n",
            "\n",
            "Epoch 00040: loss improved from 1.42998 to 1.41920, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.040.h5\n",
            "Epoch 41/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.4116\n",
            "Epoch 42/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.4107\n",
            "Epoch 43/50\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.4024\n",
            "Epoch 44/50\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 1.4044\n",
            "Epoch 45/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.4010\n",
            "\n",
            "Epoch 00045: loss improved from 1.41920 to 1.40104, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.045.h5\n",
            "Epoch 46/50\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.4042\n",
            "Epoch 47/50\n",
            "151352/151352 [==============================] - 304s 2ms/step - loss: 1.3917\n",
            "Epoch 48/50\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3911\n",
            "Epoch 49/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.3867\n",
            "Epoch 50/50\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.3891\n",
            "\n",
            "Epoch 00050: loss improved from 1.40104 to 1.38906, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.050.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7e0f41588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzYsxtXOnEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a575d83-d1ca-4f81-a4a7-f666c089fc20"
      },
      "source": [
        "# training for last 25 epochs\n",
        "model.fit(X, y, batch_size=128, epochs=25, callbacks=callback_list)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.3885\n",
            "Epoch 2/25\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.3811\n",
            "Epoch 3/25\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.3755\n",
            "Epoch 4/25\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.3766\n",
            "Epoch 5/25\n",
            "151352/151352 [==============================] - 305s 2ms/step - loss: 1.3715\n",
            "\n",
            "Epoch 00005: loss improved from 1.38906 to 1.37151, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.005.h5\n",
            "Epoch 6/25\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.3678\n",
            "Epoch 7/25\n",
            "151352/151352 [==============================] - 309s 2ms/step - loss: 1.3688\n",
            "Epoch 8/25\n",
            "151352/151352 [==============================] - 315s 2ms/step - loss: 1.3635\n",
            "Epoch 9/25\n",
            "151352/151352 [==============================] - 314s 2ms/step - loss: 1.3593\n",
            "Epoch 10/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.3618\n",
            "\n",
            "Epoch 00010: loss improved from 1.37151 to 1.36184, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.010.h5\n",
            "Epoch 11/25\n",
            "151352/151352 [==============================] - 312s 2ms/step - loss: 1.3626\n",
            "Epoch 12/25\n",
            "151352/151352 [==============================] - 311s 2ms/step - loss: 1.3554\n",
            "Epoch 13/25\n",
            "151352/151352 [==============================] - 306s 2ms/step - loss: 1.3569\n",
            "Epoch 14/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3525\n",
            "Epoch 15/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3491\n",
            "\n",
            "Epoch 00015: loss improved from 1.36184 to 1.34911, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.015.h5\n",
            "Epoch 16/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3434\n",
            "Epoch 17/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3496\n",
            "Epoch 18/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3385\n",
            "Epoch 19/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3389\n",
            "Epoch 20/25\n",
            "151352/151352 [==============================] - 302s 2ms/step - loss: 1.3364\n",
            "\n",
            "Epoch 00020: loss improved from 1.34911 to 1.33637, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.020.h5\n",
            "Epoch 21/25\n",
            "151352/151352 [==============================] - 303s 2ms/step - loss: 1.3311\n",
            "Epoch 22/25\n",
            "151352/151352 [==============================] - 302s 2ms/step - loss: 1.3325\n",
            "Epoch 23/25\n",
            "151352/151352 [==============================] - 302s 2ms/step - loss: 1.3317\n",
            "Epoch 24/25\n",
            "151352/151352 [==============================] - 301s 2ms/step - loss: 1.3293\n",
            "Epoch 25/25\n",
            "151352/151352 [==============================] - 301s 2ms/step - loss: 1.3293\n",
            "\n",
            "Epoch 00025: loss improved from 1.33637 to 1.32930, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP3/assgn2_saved_models/lstm_model.025.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7e0fa98d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcrmonek381g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"LSTM_epochs100.h5\"\n",
        "save_dir = os.path.join(os.getcwd(), 'assgn2_saved_models')\n",
        "filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "model.save(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzTydvuQv5K",
        "colab_type": "code",
        "outputId": "397e36b7-aeb7-40dd-d465-91f5e2ad663e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "pattern = \" \"*98 + \"my\"\n",
        "pattern = [char_to_int[char] for char in pattern] \n",
        "print(pattern)\n",
        "# print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7MUpdZ8W_sH",
        "colab_type": "code",
        "outputId": "8fdb2126-c941-4ebc-9306-cfb2c9d755c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''\n",
        "Somehow it generates only gibberish which doesn't seem to be expected as \n",
        "I have only trained for only 100 epochs.\n",
        "'''\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern =np.append(pattern, index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spee  ir eart w mnew snlch  a datdle asg tuoetuid asdhed fosweh mn whe anlce suoetcisedy trsroainid oitsoasdhdo t oerterid atpearert os eiang anlce suoyis tuoetcide fosserteilnedtt oitstrsttide potutruide tenaibeduttasti doe tetmact btee ectesty oeanine tuoatce e utatdl ioswertesialncdllni oitsoe crdhes sroy iooverce stoetmsn  o atehytllan ceoott fosment wolnpidyued iori midtid atpeeredt bbout ioestsrrrid b  oeane tepermals alf dony gispers bnlcesttrebteh oussrid ioamidrpy fosuesie tuoetciri l e\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuSXDmIL1uiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for other approach that I was trying \n",
        "# padding sequences per batch didn't work because of too much ram consumption\n",
        "# I'm using authors way of creating batches\n",
        "\n",
        "\n",
        "# def generate_sequences(text, batch_size=4):\n",
        "#     X_list = []\n",
        "#     y_list = []\n",
        "#     total_batches = len(text)/batch_size if len(text)%batch_size == 0 else (len(text)//batch_size+1)\n",
        "#     for line_num in range(total_batches):\n",
        "#         batch_lines = text[line_num*batch_size : (line_num+1)*batch_size]\n",
        "#         x = [] \n",
        "#         y = []\n",
        "#         for line in batch_lines:\n",
        "#             char_list = list(line)\n",
        "#             for i in range(1, (len(char_list)-1)):\n",
        "#                 seq_in = char_list[:i]\n",
        "#                 seq_out = char_list[i+1]\n",
        "#                 x.append([char_to_int[char] for char in seq_in])\n",
        "#                 y.append(char_to_int[seq_out])\n",
        "# #                 print(x)\n",
        "#         max_sequence_len = max(list(map(len, x)))\n",
        "#         x = np.array(pad_sequences(x, maxlen=max_sequence_len, padding=\"pre\", value=-10))\n",
        "#         x = x[:, :, np.newaxis]\n",
        "#         y = ku.to_categorical(y, num_classes=n_vocab)\n",
        "#         X_list.append(x)\n",
        "#         y_list.append(y)\n",
        "#     return X_list, y_list   \n",
        "  \n",
        "  \n",
        "# X, y = generate_sequences(raw_text)\n",
        "# print(X[1].shape)\n",
        "# print(y[1].shape)\n",
        "\n",
        "# print(len(X))\n",
        "\n",
        "# class My_Generator(ku.Sequence):\n",
        "#     'Generates data for Keras'\n",
        "#     def __init__(self, X, y, batch_size=1, shuffle=True):\n",
        "#         'Initialization'\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "#         self.batch_size = batch_size\n",
        "#         self.shuffle = shuffle\n",
        "#         self.on_epoch_end()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         'Denotes the number of batches per epoch'\n",
        "#         return int(np.ceil(len(self.y)/self.batch_size))\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.__data_generation(index)\n",
        "        \n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         'Shuffles indexes after each epoch'\n",
        "#         self.indexes = np.arange(len(self.y))\n",
        "#         if self.shuffle == True:\n",
        "#             np.random.shuffle(self.indexes)\n",
        "\n",
        "#     def __data_generation(self, index):\n",
        "            \n",
        "#         batch_x = self.X[index * self.batch_size:(index+1) * self.batch_size]\n",
        "#         batch_y = self.y[index * self.batch_size:(index+1) * self.batch_size]\n",
        "        \n",
        "#         return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}