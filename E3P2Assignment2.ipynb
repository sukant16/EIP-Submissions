{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E3P2Assignment2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukant16/EIP-Submissions/blob/master/E3P2Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrVGTfHPtryf",
        "colab_type": "code",
        "outputId": "2d1fbd97-5307-4af5-bcc2-35db3ab88c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget \"https://www.gutenberg.org/files/11/11-0.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 15:02:31--  https://www.gutenberg.org/files/11/11-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173595 (170K) [text/plain]\n",
            "Saving to: ‘11-0.txt.1’\n",
            "\n",
            "\r11-0.txt.1            0%[                    ]       0  --.-KB/s               \r11-0.txt.1          100%[===================>] 169.53K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-07-26 15:02:31 (1.14 MB/s) - ‘11-0.txt.1’ saved [173595/173595]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0BrWk0G-3-s",
        "colab_type": "code",
        "outputId": "6ba1b0cf-dcfc-4293-f53a-a5f214aa87a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Masking, LSTM, InputLayer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils, Sequence\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weia5vQz_MTR",
        "colab_type": "code",
        "outputId": "4a4daa8a-c254-4984-cb8f-02b9b18bc8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "11-0.txt  11-0.txt.1  assgn2_saved_models  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1huxkZzo-7hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "filename = \"11-0.txt\"\n",
        "with open(filename, 'r') as f:\n",
        "  text = f.read().lower().replace(\"\\n\", \" \")\n",
        "\n",
        "# removing punctuations   \n",
        "text = text.split(\".\")\n",
        "text = [re.sub(r'[^\\w\\s]+', \"\", line) for line in text]\n",
        "raw_text = [re.sub(r' +', \" \", line) for line in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2LSDMP2-7dN",
        "colab_type": "code",
        "outputId": "7d3237dc-fb22-4293-98bb-63c7488fe19e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "raw_text[:4]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project gutenbergs alices adventures in wonderland by lewis carroll this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever',\n",
              " ' you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at www',\n",
              " 'gutenberg',\n",
              " 'org title alices adventures in wonderland author lewis carroll posting date june 25 2008 ebook 11 release date march 1994 last updated october 6 2016 language english character set encoding utf8 start of this project gutenberg ebook alices adventures in wonderland alices adventures in wonderland lewis carroll the millennium fulcrum edition 3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xX9XA15-7Yr",
        "colab_type": "code",
        "outputId": "6cb07678-b52e-4b7e-c6f8-777d76f8c6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "chars = sorted(list({char for line in raw_text for char in line}))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = sum(map(len, raw_text))\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  152579\n",
            "Total Vocab:  38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1mzIHnJC01Y",
        "colab_type": "code",
        "outputId": "92f39146-2ddf-4aed-e690-8133e47672f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# removing those sentences where there are only 1 or 2 characters\n",
        "raw_text = [line for line in raw_text if len(line)>2]\n",
        "min(map(len, raw_text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN_A1BqV-7O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "First, I tried that each batch should have some fixed number of sentences and \n",
        "then converting them into sequences. However, despite all the hacks I could try, \n",
        "it always went OOM due to huge matrix size for some batches. \n",
        "So, below, I have taken fixed length of 100, taking character one by one till \n",
        "the point I reach the 100th character in the sentence (I am pre-padding those \n",
        "sequences where the length is less than 100) and after 100th character I start \n",
        "taking sliding window of 100.  \n",
        "'''\n",
        "seq_len = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for line in raw_text:\n",
        "  if len(line) <= 100:\n",
        "#     char_list = list(line)\n",
        "    for i in range(1, len(line)):\n",
        "      seq_in = line[:i]\n",
        "      seq_out = line[i]\n",
        "      dataX.append([char_to_int[char] for char in seq_in])\n",
        "      dataY.append(char_to_int[seq_out])\n",
        "  else:\n",
        "#     num_lines = np.ceil(len(line)/seq_len)\n",
        "    for i in range(1, len(line)):\n",
        "      if i <= 100:\n",
        "        seq_in = line[:i]\n",
        "        seq_out = line[i]\n",
        "      else:\n",
        "        seq_in = line[(i-seq_len):i]\n",
        "        seq_out = line[i]\n",
        "      dataX.append([char_to_int[char] for char in seq_in])\n",
        "      dataY.append(char_to_int[seq_out])\n",
        "dataX = pad_sequences(dataX, maxlen=seq_len, padding='pre', value=0)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KawdMfxnLu0V",
        "colab_type": "code",
        "outputId": "d9be692f-2c7c-4d6f-98c2-14f1f7d453da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(dataX), len(dataX[1]))\n",
        "print(len(dataY))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "151352 100\n",
            "151352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AF0MfPELSIq",
        "colab_type": "code",
        "outputId": "ee781bf7-dd89-4d2c-aec1-e140b40dc2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X = np.reshape(dataX, (len(dataX), seq_len, 1))\n",
        "X = X/float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(151352, 100, 1)\n",
            "(151352, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29EKMTtzDaf6",
        "colab_type": "code",
        "outputId": "544d71d3-8c65-4e43-f985-4adea0997126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 15:03:28.304238 140416076732288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 15:03:28.353207 140416076732288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 15:03:28.360413 140416076732288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 15:03:28.379365 140416076732288 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 15:03:28.415311 140416076732288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_1 (Dropout)          (None, 100, 1)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 38)                9766      \n",
            "=================================================================\n",
            "Total params: 799,270\n",
            "Trainable params: 799,270\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VboA_GkwDdO2",
        "colab_type": "code",
        "outputId": "08894414-b8f1-4b85-c947-63aec5a373cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "'''\n",
        "Due to lack of time, I am only training for 7 epohcs as of now. I'll submit the \n",
        "assignment again on 27th July when the training finishes for 100 epochs.\n",
        "'''\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'assgn2_saved_models')\n",
        "model_name = 'lstm_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True, period=5)\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "model.fit(X, y, batch_size=128, epochs=7, callbacks=callbacks)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "151352/151352 [==============================] - 965s 6ms/step - loss: 2.7207\n",
            "Epoch 2/7\n",
            "151352/151352 [==============================] - 970s 6ms/step - loss: 2.5354\n",
            "Epoch 3/7\n",
            "151352/151352 [==============================] - 969s 6ms/step - loss: 2.3718\n",
            "Epoch 4/7\n",
            "151352/151352 [==============================] - 951s 6ms/step - loss: 2.2619\n",
            "Epoch 5/7\n",
            "151352/151352 [==============================] - 966s 6ms/step - loss: 2.1835\n",
            "Epoch 6/7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "151352/151352 [==============================] - 968s 6ms/step - loss: 2.1213\n",
            "Epoch 7/7\n",
            "151352/151352 [==============================] - 968s 6ms/step - loss: 2.0660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb490bfe358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF_NGZ5P3f2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2ac09a8c-0159-4e6b-b50e-13fcc6e3f376"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW5qEWcB3UcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/EIP3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcrmonek381g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"LSTM_epochs7.h5\"\n",
        "save_dir = os.path.join(os.getcwd(), 'assgn2_saved_models')\n",
        "filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "model.save(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzTydvuQv5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cf30ee17-1ebe-41d3-888f-f7d909e6eed5"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"                 i never went to him the mock turtle said with a sigh he taught laughing and grief th \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7MUpdZ8W_sH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "69087ceb-45ec-4b40-bc88-60a299c4fc63"
      },
      "source": [
        "'''\n",
        "Currently it generates only gibberish which is quite expected as \n",
        "I have only trained for only 7 epochs. I'll train for 100 epochs and\n",
        "check what difference it makes.\n",
        "'''\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern =np.append(pattern, index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " toe sf tuoet ontercerg tuoet on tu treee tf toeceed toe sf tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor tuoeer gor \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuSXDmIL1uiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for other approach that I was trying \n",
        "# padding sequences per batch didn't work because of too much ram consumption\n",
        "# I'm using authors way of creating batches\n",
        "\n",
        "\n",
        "# def generate_sequences(text, batch_size=4):\n",
        "#     X_list = []\n",
        "#     y_list = []\n",
        "#     total_batches = len(text)/batch_size if len(text)%batch_size == 0 else (len(text)//batch_size+1)\n",
        "#     for line_num in range(total_batches):\n",
        "#         batch_lines = text[line_num*batch_size : (line_num+1)*batch_size]\n",
        "#         x = [] \n",
        "#         y = []\n",
        "#         for line in batch_lines:\n",
        "#             char_list = list(line)\n",
        "#             for i in range(1, (len(char_list)-1)):\n",
        "#                 seq_in = char_list[:i]\n",
        "#                 seq_out = char_list[i+1]\n",
        "#                 x.append([char_to_int[char] for char in seq_in])\n",
        "#                 y.append(char_to_int[seq_out])\n",
        "# #                 print(x)\n",
        "#         max_sequence_len = max(list(map(len, x)))\n",
        "#         x = np.array(pad_sequences(x, maxlen=max_sequence_len, padding=\"pre\", value=-10))\n",
        "#         x = x[:, :, np.newaxis]\n",
        "#         y = ku.to_categorical(y, num_classes=n_vocab)\n",
        "#         X_list.append(x)\n",
        "#         y_list.append(y)\n",
        "#     return X_list, y_list   \n",
        "  \n",
        "  \n",
        "# X, y = generate_sequences(raw_text)\n",
        "# print(X[1].shape)\n",
        "# print(y[1].shape)\n",
        "\n",
        "# print(len(X))\n",
        "\n",
        "# class My_Generator(ku.Sequence):\n",
        "#     'Generates data for Keras'\n",
        "#     def __init__(self, X, y, batch_size=1, shuffle=True):\n",
        "#         'Initialization'\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "#         self.batch_size = batch_size\n",
        "#         self.shuffle = shuffle\n",
        "#         self.on_epoch_end()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         'Denotes the number of batches per epoch'\n",
        "#         return int(np.ceil(len(self.y)/self.batch_size))\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.__data_generation(index)\n",
        "        \n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         'Shuffles indexes after each epoch'\n",
        "#         self.indexes = np.arange(len(self.y))\n",
        "#         if self.shuffle == True:\n",
        "#             np.random.shuffle(self.indexes)\n",
        "\n",
        "#     def __data_generation(self, index):\n",
        "            \n",
        "#         batch_x = self.X[index * self.batch_size:(index+1) * self.batch_size]\n",
        "#         batch_y = self.y[index * self.batch_size:(index+1) * self.batch_size]\n",
        "        \n",
        "#         return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}